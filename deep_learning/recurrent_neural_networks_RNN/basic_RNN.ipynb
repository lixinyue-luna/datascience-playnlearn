{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's in the notebook\n",
    "- [Build an RNN from Scratch using Numpy](#scratch)\n",
    "\n",
    "Recurrent Neural Networks (RNN) are effective for sequence tasks such as natural language processing and time-series forecasting. RNNs can read inputs $x^{<t>}$ one at a time and remember some information or context through the hidden layer activations that get passed from one time-step to the next. A uni-directional RNN takes information from the past to process future inputs. A bi-directional RNN can take context from both the past and the future. \n",
    "\n",
    "Notation used in this notebook:\n",
    "- Layers:\n",
    "    - $a^{[l]}$: activation of the $l$-th layer\n",
    "    - $a^{[l]}_i$: the $i$-th activation unit in layer $l$\n",
    "    - $a^{<t>}$: activation at the $t$-th timestep\n",
    "    - $W^{[l]}$: weights of the $l$-th layer\n",
    "    - $b^{[l]}$: bias of the $l$-th layer\n",
    "- Inputs:\n",
    "    - $x^{(i)}$: the $i$-th training example\n",
    "    - $x^{<t>}$: the input x at the $t$-th timestep\n",
    "    - $x^{(i)<t>}$: the input at the $t$-th timestep of the $i$-th example    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='scratch'></a>Build an RNN from Scratch using Numpy\n",
    "The basic strucuture of a simple RNN with basic RNN cells is as below:\n",
    "<img src=\"images/rnn (1).png\" style=\"width:300;height:300px;\">\n",
    "<caption><center> **Figure 1**: Basic RNN model. *Source*: deeplearning.ai</center></caption>\n",
    "\n",
    "**A basic RNN-cell**:\n",
    "<img src=\"images/rnn_step_forward.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 2**: Basic RNN cell. *Source*: deeplearning.ai </center></caption>\n",
    "This RNN cell takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$.\n",
    "\n",
    "\n",
    "**Backpropagation**:\n",
    "<img src=\"images/rnn_cell_backprop.png\" style=\"width:500;height:300px;\"> <br>\n",
    "<caption><center> **Figure 4**: RNN-cell's backward pass. *Source*: deeplearning.ai </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# helper functions\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def initialize_adam(parameters) :\n",
    "    '''\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: 'dW1', 'db1', ..., 'dWL', 'dbL' \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters\n",
    "    \n",
    "    :params parameters: python dictionary containing parameters\n",
    "                - parameters['W' + str(l)] = Wl\n",
    "                - parameters['b' + str(l)] = bl\n",
    "    \n",
    "    :return:\n",
    "    v: python dictionary that contains the exponentially weighted average of the gradient\n",
    "    s: python dictionary that contains the exponentially weighted average of the squared gradient\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: 'parameters'. Outputs: 'v, s'.\n",
    "    for l in range(L):\n",
    "        v['dW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v['db' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "        s['dW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        s['db' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "\n",
    "    return v, s\n",
    "\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    '''\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    :params parameters: python dictionary containing parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    :params grads: python dictionary containing gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    :params v: Adam variable, moving average of the first gradient, python dictionary\n",
    "    :params s: Adam variable, moving average of the squared gradient, python dictionary\n",
    "    :params learning_rate: the learning rate, scalar.\n",
    "    :params beta1: Exponential decay hyperparameter for the first moment estimates \n",
    "    :params beta2: Exponential decay hyperparameter for the second moment estimates \n",
    "    :params epsilon: hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    :return:\n",
    "    parameters: python dictionary containing your updated parameters \n",
    "    v: Adam variable, moving average of the first gradient, python dictionary\n",
    "    s: Adam variable, moving average of the squared gradient, python dictionary\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients \n",
    "        v['dW' + str(l+1)] = beta1 * v['dW' + str(l+1)] + (1 - beta1) * grads['dW' + str(l+1)] \n",
    "        v['db' + str(l+1)] = beta1 * v['db' + str(l+1)] + (1 - beta1) * grads['db' + str(l+1)] \n",
    "\n",
    "        # Compute bias-corrected first moment estimate\n",
    "        v_corrected['dW' + str(l+1)] = v['dW' + str(l+1)] / (1 - beta1**t)\n",
    "        v_corrected['db' + str(l+1)] = v['db' + str(l+1)] / (1 - beta1**t)\n",
    "\n",
    "        # Moving average of the squared gradients\n",
    "        s['dW' + str(l+1)] = beta2 * s['dW' + str(l+1)] + (1 - beta2) * (grads['dW' + str(l+1)] ** 2)\n",
    "        s['db' + str(l+1)] = beta2 * s['db' + str(l+1)] + (1 - beta2) * (grads['db' + str(l+1)] ** 2)\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate\n",
    "        s_corrected['dW' + str(l+1)] = s['dW' + str(l+1)] / (1 - beta2 ** t)\n",
    "        s_corrected['db' + str(l+1)] = s['db' + str(l+1)] / (1 - beta2 ** t)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters['W' + str(l+1)] = parameters['W' + str(l+1)] - learning_rate * v_corrected['dW' + str(l+1)] / np.sqrt(s_corrected['dW' + str(l+1)] + epsilon)\n",
    "        parameters['b' + str(l+1)] = parameters['b' + str(l+1)] - learning_rate * v_corrected['db' + str(l+1)] / np.sqrt(s_corrected['db' + str(l+1)] + epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward step\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    '''\n",
    "    Implements a single forward step of the basic RNN-cell described in Figure 2\n",
    "    \n",
    "    :params xt: numpy array of shape (n_x, m) representing input at timestep t\n",
    "    :params a_prev: numpy array of shape (n_a, m) representing hidden state at timestep t-1\n",
    "    :params parameters: python dictionary containing layer parameters:\n",
    "                        {Wax: numpy array of shape (n_a, n_x) representing the weight matrix multiplying the input xt,\n",
    "                         Waa: numpy array of shape (n_a, n_a) representing the weight matrix multiplying the hidden state a_prev,\n",
    "                         Wya: numpy array of shape (n_y, n_a) representing the weight matrix relating to the hidden state to the output,\n",
    "                         ba:  numpy array of shape (n_a, 1) representing the bias,\n",
    "                         by:  numpy array of shape (n_y, 1) representing the bias relating the hidden-state to the output}\n",
    "\n",
    "    :return: \n",
    "    a_next: next hidden state of shape (n_a, m)\n",
    "    yt_pred: prediction at timestep t of shape (n_y, m)\n",
    "    cache: tuple of values (a_next, a_prev, xt, parameters) needed for the backward pass\n",
    "    '''\n",
    "    Wax = parameters['Wax']\n",
    "    Waa = parameters['Waa']\n",
    "    Wya = parameters['Wya']\n",
    "    ba = parameters['ba']\n",
    "    by = parameters['by']\n",
    "    \n",
    "    # compute the hidden state a<t> with tanh activation\n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    \n",
    "    # compute the prediction ŷ⟨t⟩\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)\n",
    "    \n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    '''\n",
    "    Implement the forward propagation of the recurrent neural network.\n",
    "\n",
    "    :params x: Input data for every time-step, of shape (n_x, m, T_x)\n",
    "    :params a0: Initial hidden state, of shape (n_a, m)\n",
    "    :params parameters: python dictionary containing layer parameters:\n",
    "                        {Wax: numpy array of shape (n_a, n_x) representing the weight matrix multiplying the input xt,\n",
    "                         Waa: numpy array of shape (n_a, n_a) representing the weight matrix multiplying the hidden state a_prev,\n",
    "                         Wya: numpy array of shape (n_y, n_a) representing the weight matrix relating to the hidden state to the output,\n",
    "                         ba:  numpy array of shape (n_a, 1) representing the bias,\n",
    "                         by:  numpy array of shape (n_y, 1) representing the bias relating the hidden-state to the output}\n",
    "\n",
    "    :return:\n",
    "    a: Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred: Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches: tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    '''\n",
    "    # Initialize 'caches' which will contain the list of all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and parameters['Wya']\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters['Wya'].shape\n",
    "    \n",
    "    # initialize a and y with zeros\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next\n",
    "    a_next = a0\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters)\n",
    "        # Save the value of the new next hidden state in a\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the prediction in y\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        # Append cache to caches \n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next, cache):\n",
    "    '''\n",
    "    Implements the backward pass for the RNN-cell (single time-step) in Figure 3.\n",
    "\n",
    "    :params da_next: Gradient of loss with respect to next hidden state\n",
    "    :params cache: python dictionary containing useful values (output of rnn_cell_forward())\n",
    "\n",
    "    :return:\n",
    "    gradients: python dictionary containing:\n",
    "                {dx: Gradients of input data, of shape (n_x, m),\n",
    "                 da_prev: Gradients of previous hidden state, of shape (n_a, m),\n",
    "                 dWax: Gradients of input-to-hidden weights, of shape (n_a, n_x),\n",
    "                 dWaa: Gradients of hidden-to-hidden weights, of shape (n_a, n_a),\n",
    "                 dba: Gradients of bias vector, of shape (n_a, 1)}\n",
    "    '''\n",
    "    # Retrieve values from cache\n",
    "    (a_next, a_prev, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve values from parameters\n",
    "    Wax = parameters['Wax']  # (n_a, n_x)\n",
    "    Waa = parameters['Waa']  # (n_a, n_a)\n",
    "    Wya = parameters['Wya']  # (n_y, n_a)\n",
    "    ba = parameters['ba']    # (n_a, 1)\n",
    "    by = parameters['by']    # (n_y, 1)\n",
    "\n",
    "    # compute the gradient of tanh with respect to a_next\n",
    "    dtanh = da_next * (1. - np.square(np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba))) # (n_a, m)\n",
    "\n",
    "    # compute the gradient of the loss with respect to Wax\n",
    "    dxt = np.dot(Wax.T, dtanh)\n",
    "    dWax = np.dot(dtanh, xt.T)\n",
    "\n",
    "    # compute the gradient with respect to Waa\n",
    "    da_prev = np.dot(Waa.T, dtanh)\n",
    "    dWaa = np.dot(dtanh, a_prev.T)\n",
    "\n",
    "    # compute the gradient with respect to b\n",
    "    dba = np.sum(dtanh, axis=1, keepdims=True)\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "    '''\n",
    "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
    "   \n",
    "    :params da: Upstream gradients of all hidden states, of shape (n_a, m, T_x)\n",
    "    :params caches: tuple containing information from the forward pass (rnn_forward)\n",
    "    \n",
    "    :return:\n",
    "    gradients: python dictionary containing:\n",
    "            {dx: Gradients of input data, of shape (n_x, m),\n",
    "             da0: Gradients of initial hidden state, of shape (n_a, m),\n",
    "             dWax: Gradients of input-to-hidden weights, of shape (n_a, n_x),\n",
    "             dWaa: Gradients of hidden-to-hidden weights, of shape (n_a, n_a),\n",
    "             dba: Gradients of bias vector, of shape (n_a, 1)}\n",
    "    '''\n",
    "    # Retrieve values from the first cache (t=1) of caches \n",
    "    (caches, x) = caches\n",
    "    (a1, a0, x1, parameters) = caches[0]\n",
    "    \n",
    "    # Retrieve dimensions from da's and x1's shapes \n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # initialize the gradients\n",
    "    dx = np.zeros((n_x, m, T_x))\n",
    "    dWax = np.zeros((n_a, n_x))\n",
    "    dWaa = np.zeros((n_a, n_a))\n",
    "    dba = np.zeros((n_a, 1))\n",
    "    da0 = np.zeros((n_a, m))\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    \n",
    "    # Loop through all the time steps\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Compute gradients at time step t.\n",
    "        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])\n",
    "        # Retrieve derivatives from gradients\n",
    "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients['dxt'], gradients['da_prev'], gradients['dWax'], gradients['dWaa'], gradients['dba']\n",
    "        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t\n",
    "        dx[:, :, t] = dxt\n",
    "        dWax += dWaxt\n",
    "        dWaa += dWaat\n",
    "        dba += dbat\n",
    "        \n",
    "    # Set da0 to the gradient of a which has been backpropagated through all time-steps \n",
    "    da0 = da_prevt\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
